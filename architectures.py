# -*- coding: utf-8 -*-
"""architectures.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hyCeWUHnSEdURm4nguq4hXh7Ce7jzZ8g
"""

ENVIRONMENT_TYPE = "COLAB" # NONE
import os
if ENVIRONMENT_TYPE == "COLAB":
  BASE_PATH = os.path.join('/content', 'gdrive', 'MyDrive', 'Research') # Your custom file path to run in google colab
else:
  BASE_PATH = os.path.join('.') # in local machine
  pass

from tensorflow import keras
from tensorflow.keras import layers
from keras.models import Sequential
from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout
from tensorflow.keras.layers import BatchNormalization
import tensorflow

import keras.backend as K
from keras.models import Model
from keras.layers import Conv2D, MaxPool2D,  \
    Dropout, Dense, Input, concatenate,      \
    GlobalAveragePooling2D, AveragePooling2D,\
    Flatten, Activation, Add, ZeroPadding2D, ZeroPadding2D, DepthwiseConv2D, BatchNormalization, ReLU
from tensorflow.keras import activations
from tensorflow.keras.regularizers import l2

import cv2
import numpy as np
from keras.datasets import cifar10
from keras import backend as K
from keras.utils import np_utils
import math
from keras.optimizers import SGD
from keras.callbacks import LearningRateScheduler

import os
import time
import gc

def get_run_logdir(root_logdir):
    run_id = time.strftime("run_%Y_%m_%d-%H_%M_%S")
    return os.path.join(root_logdir, run_id)

# Alex Net
# Link: https://towardsdatascience.com/implementing-alexnet-cnn-architecture-using-tensorflow-2-0-and-keras-2113e090ad98
# https://www.analyticsvidhya.com/blog/2021/03/introduction-to-the-architecture-of-alexnet/#:~:text=The%20Alexnet%20has%20eight%20layers,layers%20except%20the%20output%20layer.
def create_alex_net(patience=30, classes=2):
  alex_net=Sequential()
  alex_net = keras.models.Sequential([
      keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3)),
      keras.layers.BatchNormalization(),
      keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),

      keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding="same"),
      keras.layers.BatchNormalization(),
      keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),

      keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding="same"),
      keras.layers.BatchNormalization(),

      keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding="same"),
      keras.layers.BatchNormalization(),

      keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding="same"),
      keras.layers.BatchNormalization(),
      keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),

      keras.layers.Flatten(),
      keras.layers.Dense(4096, activation='relu'),
      keras.layers.Dropout(0.5),
      keras.layers.Dense(4096, activation='relu'),
      keras.layers.Dropout(0.5),
      keras.layers.Dense(classes, activation='softmax')
  ])
  alex_net_checkpoint_filepath = os.path.join(BASE_PATH,'alex_net.hdf5')
  alex_net_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(
      filepath=alex_net_checkpoint_filepath,
      save_weights_only=False,
      monitor='val_accuracy',
      mode='auto',
      save_best_only=True,
      verbose=0,
      on_epoch_end=gc.collect())
  alex_net_root_logdir = BASE_PATH
  #run_logdir = get_run_logdir()
  #tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
  alex_net.compile(
      optimizer=tensorflow.optimizers.SGD(learning_rate=0.001), # learning_rate=0.001
      loss="categorical_crossentropy",  # sparse_categorical_crossentropy, binary_crossentropy
      metrics=[keras.metrics.CategoricalAccuracy(name="accuracy")],
  )
  early_stopping = tensorflow.keras.callbacks.EarlyStopping(
   monitor="val_loss",
    min_delta=0,
    patience=patience,
    verbose=0,
    mode="auto",
    baseline=None,
    restore_best_weights=True,
 )
  # alex_net.summary()
  return alex_net, alex_net_checkpoint_callback, alex_net_checkpoint_filepath, early_stopping

# https://towardsdatascience.com/step-by-step-vgg16-implementation-in-keras-for-beginners-a833c686ae6c
def create_vgg16_net(patience=30, classes=2):
  vgg16_net = Sequential()
  vgg16_net.add(keras.layers.Conv2D(input_shape=(227,227,3),filters=64,kernel_size=(3,3),padding="same", activation="relu"))
  vgg16_net.add(keras.layers.Conv2D(filters=64,kernel_size=(3,3),padding="same", activation="relu"))
  vgg16_net.add(keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))

  vgg16_net.add(keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
  vgg16_net.add(keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
  vgg16_net.add(keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))

  vgg16_net.add(keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
  vgg16_net.add(keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
  vgg16_net.add(keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
  vgg16_net.add(keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))

  vgg16_net.add(keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
  vgg16_net.add(keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
  vgg16_net.add(keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
  vgg16_net.add(keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))

  vgg16_net.add(keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
  vgg16_net.add(keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
  vgg16_net.add(keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
  vgg16_net.add(keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))

  vgg16_net.add(Flatten())
  vgg16_net.add(Dense(units=4096,activation="relu"))
  vgg16_net.add(Dense(units=4096,activation="relu"))
  vgg16_net.add(Dense(units=classes, activation="softmax"))

  vgg16_net_checkpoint_filepath = os.path.join(BASE_PATH,  'vgg16_net.hdf5')
  vgg16_net_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(
      filepath=vgg16_net_checkpoint_filepath,
      save_weights_only=True,
      monitor='val_loss',
      mode='auto',
      save_best_only=True,
      on_epoch_end=gc.collect())

  early_stopping = tensorflow.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0,
    patience=patience,
    verbose=0,
    mode="auto",
    baseline=None,
    restore_best_weights=True,
 )
  vgg16_net_root_logdir = BASE_PATH
  #run_logdir = get_run_logdir()
  #tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
  vgg16_net.compile(
      optimizer=tensorflow.optimizers.SGD(learning_rate=0.001), # learning_rate=0.001
      loss="categorical_crossentropy",  # sparse_categorical_crossentropy, binary_crossentropy
      metrics=[keras.metrics.CategoricalAccuracy(name="accuracy")],
  )

  # vgg16_net.summary()
  return vgg16_net, vgg16_net_checkpoint_callback, vgg16_net_checkpoint_filepath, early_stopping

def create_leaf_net(patience=30, classes=2):
  baseline_model = Sequential([
      keras.layers.Conv2D(filters=50, kernel_size=(11,11), strides=(3,3),  activation='relu', input_shape=(227,227,3)), # 5 was
      keras.layers.BatchNormalization(),
      keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),

      keras.layers.Conv2D(filters=100, kernel_size=(11,11), strides=(1,1),  activation='relu', padding="same"), # 5 was
      keras.layers.BatchNormalization(),
      keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),

      keras.layers.Conv2D(filters=150, kernel_size=(5,5), strides=(1,1),  activation='relu', padding="same"), # 5 was
      keras.layers.BatchNormalization(),


      keras.layers.Conv2D(filters=100, kernel_size=(5,5), strides=(1,1),  activation='relu', padding="same"), # 5 was
      keras.layers.BatchNormalization(),
      keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),

      keras.layers.Conv2D(filters=90, kernel_size=(3,3), strides=(1,1),  activation='relu', padding="same"), # 5 was
      keras.layers.BatchNormalization(),
      keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),

      keras.layers.Flatten(),

      keras.layers.Dense(800, activation='relu'),
      keras.layers.Dropout(0.5),
      keras.layers.Dense(800, activation='relu'),
      keras.layers.Dropout(0.5),
      keras.layers.Dense(classes, activation='softmax')
  ])



  baseline_checkpoint_filepath = os.path.join(BASE_PATH, 'leaf_net.hdf5')

  baseline_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(
      filepath=baseline_checkpoint_filepath,
      save_weights_only=True,
      monitor='val_loss',
      mode='auto',
      save_best_only=True,
      on_epoch_end=gc.collect())

  early_stopping = tensorflow.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0,
    patience=patience,
    verbose=0,
    mode="auto",
    baseline=None,
    restore_best_weights=True,
 )

  # tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
  baseline_model.compile(
      optimizer=tensorflow.optimizers.SGD(learning_rate=0.001), # learning_rate=0.001
      loss="categorical_crossentropy",  # sparse_categorical_crossentropy, binary_crossentropy
      metrics=[keras.metrics.CategoricalAccuracy(name="accuracy")],
  )
  baseline_model.summary()
  return baseline_model, baseline_checkpoint_callback, baseline_checkpoint_filepath, early_stopping

# create_leaf_net()

# https://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/

def inception_module(x,
                     filters_1x1,
                     filters_3x3_reduce,
                     filters_3x3,
                     filters_5x5_reduce,
                     filters_5x5,
                     filters_pool_proj,
                     name=None):

    kernel_init = keras.initializers.glorot_uniform()
    bias_init = keras.initializers.Constant(value=0.2)
    conv_1x1 = Conv2D(filters_1x1, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x) #

    conv_3x3 = Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x) #
    conv_3x3 = Conv2D(filters_3x3, (3, 3), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_3x3)

    conv_5x5 = Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)
    conv_5x5 = Conv2D(filters_5x5, (5, 5), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_5x5)

    pool_proj = MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)
    pool_proj = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(pool_proj)

    output = concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=3, name=name)

    return output

def decay(epoch, steps=100):
    initial_lrate = 0.01
    drop = 0.96
    epochs_drop = 8
    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
    return lrate

def create_inception_net(initial_lrate = 0.01, patience=30, classes=2):
  kernel_init = keras.initializers.glorot_uniform()
  bias_init = keras.initializers.Constant(value=0.2)
  input_layer = Input(shape=(227, 227, 3))
  x = Conv2D(64, (7, 7), padding='same', strides=(2, 2), activation='relu', name='conv_1_7x7/2', kernel_initializer=kernel_init, bias_initializer=bias_init)(input_layer)
  x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_1_3x3/2')(x)
  x = Conv2D(64, (1, 1), padding='same', strides=(1, 1), activation='relu', name='conv_2a_3x3/1')(x)
  x = Conv2D(192, (3, 3), padding='same', strides=(1, 1), activation='relu', name='conv_2b_3x3/1')(x)
  x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_2_3x3/2')(x)

  x = inception_module(x,
                      filters_1x1=64,
                      filters_3x3_reduce=96,
                      filters_3x3=128,
                      filters_5x5_reduce=16,
                      filters_5x5=32,
                      filters_pool_proj=32,
                      name='inception_3a')

  x = inception_module(x,
                      filters_1x1=128,
                      filters_3x3_reduce=128,
                      filters_3x3=192,
                      filters_5x5_reduce=32,
                      filters_5x5=96,
                      filters_pool_proj=64,
                      name='inception_3b')

  x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_3_3x3/2')(x)

  x = inception_module(x,
                      filters_1x1=192,
                      filters_3x3_reduce=96,
                      filters_3x3=208,
                      filters_5x5_reduce=16,
                      filters_5x5=48,
                      filters_pool_proj=64,
                      name='inception_4a')


  x1 = AveragePooling2D((5, 5), strides=3)(x)
  x1 = Conv2D(128, (1, 1), padding='same', activation='relu')(x1)
  x1 = Flatten()(x1)
  x1 = Dense(1024, activation='relu')(x1)
  x1 = Dropout(0.7)(x1)
  x1 = Dense(classes, activation='softmax', name='auxilliary_output_1')(x1)

  x = inception_module(x,
                      filters_1x1=160,
                      filters_3x3_reduce=112,
                      filters_3x3=224,
                      filters_5x5_reduce=24,
                      filters_5x5=64,
                      filters_pool_proj=64,
                      name='inception_4b')

  x = inception_module(x,
                      filters_1x1=128,
                      filters_3x3_reduce=128,
                      filters_3x3=256,
                      filters_5x5_reduce=24,
                      filters_5x5=64,
                      filters_pool_proj=64,
                      name='inception_4c')

  x = inception_module(x,
                      filters_1x1=112,
                      filters_3x3_reduce=144,
                      filters_3x3=288,
                      filters_5x5_reduce=32,
                      filters_5x5=64,
                      filters_pool_proj=64,
                      name='inception_4d')


  x2 = AveragePooling2D((5, 5), strides=3)(x)
  x2 = Conv2D(128, (1, 1), padding='same', activation='relu')(x2)
  x2 = Flatten()(x2)
  x2 = Dense(1024, activation='relu')(x2)
  x2 = Dropout(0.7)(x2)
  x2 = Dense(classes, activation='softmax', name='auxilliary_output_2')(x2)

  x = inception_module(x,
                      filters_1x1=256,
                      filters_3x3_reduce=160,
                      filters_3x3=320,
                      filters_5x5_reduce=32,
                      filters_5x5=128,
                      filters_pool_proj=128,
                      name='inception_4e')

  x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_4_3x3/2')(x)

  x = inception_module(x,
                      filters_1x1=256,
                      filters_3x3_reduce=160,
                      filters_3x3=320,
                      filters_5x5_reduce=32,
                      filters_5x5=128,
                      filters_pool_proj=128,
                      name='inception_5a')

  x = inception_module(x,
                      filters_1x1=384,
                      filters_3x3_reduce=192,
                      filters_3x3=384,
                      filters_5x5_reduce=48,
                      filters_5x5=128,
                      filters_pool_proj=128,
                      name='inception_5b')

  x = GlobalAveragePooling2D(name='avg_pool_5_3x3/1')(x)

  x = Dropout(0.4)(x)

  x = Dense(classes, activation='softmax')(x)
  model = Model(input_layer, [x, x1, x2], name='inception_v1')

  model_checkpoint_filepath = os.path.join(BASE_PATH, 'inception_net.hdf5')

  model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(
      filepath=model_checkpoint_filepath,
      save_weights_only=True,
      monitor='val_loss',
      mode='auto',
      save_best_only=True,
      on_epoch_end=gc.collect())

  early_stopping = tensorflow.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0,
    patience=patience,
    verbose=0,
    mode="auto",
    baseline=None,
    restore_best_weights=True,
 )
  sgd = SGD(lr=initial_lrate, momentum=0.9, nesterov=False)
  lr_sc = LearningRateScheduler(decay, verbose=0)
  model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'], loss_weights=[1, 0.3, 0.3], optimizer=sgd, metrics=['accuracy'])
  model.summary()
  return model, model_checkpoint_callback, model_checkpoint_filepath, early_stopping

# https://towardsdatascience.com/understand-and-implement-resnet-50-with-tensorflow-2-0-1190b9b52691
def res_identity(x, filters):
  #renet block where dimension doesnot change.
  #The skip connection is just simple identity conncection
  #we will have 3 blocks and then input will be added

  x_skip = x # this will be used for addition with the residual block
  f1, f2 = filters

  #first block
  x = Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)
  x = BatchNormalization()(x)
  x = Activation(activations.relu)(x)

  #second block # bottleneck (but size kept same with padding)
  x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001))(x)
  x = BatchNormalization()(x)
  x = Activation(activations.relu)(x)

  # third block activation used after adding the input
  x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)
  x = BatchNormalization()(x)
  # x = Activation(activations.relu)(x)

  # add the input
  x = Add()([x, x_skip])
  x = Activation(activations.relu)(x)
  return x

def res_conv(x, s, filters):
  '''
  here the input size changes'''
  x_skip = x
  f1, f2 = filters

  # first block
  x = Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001))(x)
  # when s = 2 then it is like downsizing the feature map
  x = BatchNormalization()(x)
  x = Activation(activations.relu)(x)

  # second block
  x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.001))(x)
  x = BatchNormalization()(x)
  x = Activation(activations.relu)(x)

  #third block
  x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)
  x = BatchNormalization()(x)

  # shortcut
  x_skip = Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.001))(x_skip)
  x_skip = BatchNormalization()(x_skip)

  # add
  x = Add()([x, x_skip])
  x = Activation(activations.relu)(x)

  return x

def create_reset50(classes=2, patience=30):
  input_im = Input(shape=(227, 227, 3))
  x = ZeroPadding2D(padding=(3, 3))(input_im)

  # 1st stage
  # here we perform maxpooling, see the figure above

  x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2))(x)
  x = BatchNormalization()(x)
  x = Activation(activations.relu)(x)
  x = MaxPooling2D((3, 3), strides=(2, 2))(x)

  #2nd stage
  # frm here on only conv block and identity block, no pooling

  x = res_conv(x, s=1, filters=(64, 256))
  x = res_identity(x, filters=(64, 256))
  x = res_identity(x, filters=(64, 256))

  # 3rd stage

  x = res_conv(x, s=2, filters=(128, 512))
  x = res_identity(x, filters=(128, 512))
  x = res_identity(x, filters=(128, 512))
  x = res_identity(x, filters=(128, 512))

  # 4th stage

  x = res_conv(x, s=2, filters=(256, 1024))
  x = res_identity(x, filters=(256, 1024))
  x = res_identity(x, filters=(256, 1024))
  x = res_identity(x, filters=(256, 1024))
  x = res_identity(x, filters=(256, 1024))
  x = res_identity(x, filters=(256, 1024))

  # 5th stage

  x = res_conv(x, s=2, filters=(512, 2048))
  x = res_identity(x, filters=(512, 2048))
  x = res_identity(x, filters=(512, 2048))

  # ends with average pooling and dense connection

  x = AveragePooling2D((2, 2), padding='same')(x)

  x = Flatten()(x)
  x = Dense(classes, activation='softmax', kernel_initializer='he_normal')(x) #multi-class

  # define the model
  model = Model(inputs=input_im, outputs=x)

  model_checkpoint_filepath = os.path.join(BASE_PATH, 'res_net50.hdf5')

  model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(
      filepath=model_checkpoint_filepath,
      save_weights_only=True,
      monitor='val_loss',
      mode='auto',
      save_best_only=True,
      on_epoch_end=gc.collect())

  early_stopping = tensorflow.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0,
    patience=patience,
    verbose=0,
    mode="auto",
    baseline=None,
    restore_best_weights=True,
 )
  # tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
  model.compile(
      optimizer=tensorflow.optimizers.SGD(learning_rate=0.001), # learning_rate=0.001
      loss="categorical_crossentropy",  # sparse_categorical_crossentropy, binary_crossentropy
      metrics=[keras.metrics.CategoricalAccuracy(name="accuracy")],
  )
  model.summary()
  return model, model_checkpoint_callback, model_checkpoint_filepath, early_stopping

# https://medium.com/analytics-vidhya/creating-mobilenetsv2-with-tensorflow-from-scratch-c85eb8605342
def expansion_block(x,t,filters,block_id):
    prefix = 'block_{}_'.format(block_id)
    total_filters = t*filters
    x = Conv2D(total_filters,1,padding='same',use_bias=False, name = prefix +'expand')(x)
    x = BatchNormalization(name=prefix +'expand_bn')(x)
    x = ReLU(6,name = prefix +'expand_relu')(x)
    return x
def depthwise_block(x,stride,block_id):
    prefix = 'block_{}_'.format(block_id)
    x = DepthwiseConv2D(3,strides=(stride,stride),padding ='same', use_bias = False, name = prefix + 'depthwise_conv')(x)
    x = BatchNormalization(name=prefix +'dw_bn')(x)
    x = ReLU(6,name = prefix +'dw_relu')(x)
    return x
def projection_block(x,out_channels,block_id):
    prefix = 'block_{}_'.format(block_id)
    x = Conv2D(filters=out_channels,kernel_size = 1,   padding='same',use_bias=False,name= prefix + 'compress')(x)
    x = BatchNormalization(name=prefix +'compress_bn')(x)
    return x
def Bottleneck(x,t,filters, out_channels,stride,block_id):
    y = expansion_block(x,t,filters,block_id)
    y = depthwise_block(y,stride,block_id)
    y = projection_block(y, out_channels,block_id)
    if y.shape[-1]==x.shape[-1]:
       y = Add()([x,y])
    return y
def create_mobilenetv2(classes=2, patience=30):
  input = Input(shape=(227, 227, 3))
  x = Conv2D(32,3,strides=(2,2),padding='same', use_bias=False)(input)
  x = BatchNormalization(name='conv1_bn')(x)
  x = ReLU(6, name='conv1_relu')(x)

  # 17 Bottlenecks
  x = depthwise_block(x,stride=1,block_id=1)
  x = projection_block(x, out_channels=16,block_id=1)
  x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 24, stride = 2,block_id = 2)
  x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 24, stride = 1,block_id = 3)
  x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 32, stride = 2,block_id = 4)
  x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 32, stride = 1,block_id = 5)
  x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 32, stride = 1,block_id = 6)
  x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 2,block_id = 7)
  x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 1,block_id = 8)
  x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 1,block_id = 9)
  x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 1,block_id = 10)
  x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 96, stride = 1,block_id = 11)
  x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 96, stride = 1,block_id = 12)
  x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 96, stride = 1,block_id = 13)
  x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 160, stride = 2,block_id = 14)
  x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 160, stride = 1,block_id = 15)
  x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 160, stride = 1,block_id = 16)
  x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 320, stride = 1,block_id = 17)

  x = Conv2D(filters = 1280,kernel_size = 1,padding='same',use_bias=False, name = 'last_conv')(x)
  x = BatchNormalization(name='last_bn')(x)
  x = ReLU(6,name='last_relu')(x)
  x = GlobalAveragePooling2D(name='global_average_pool')(x)
  output = Dense(classes,activation='softmax')(x)
  model = Model(input, output)
  model_checkpoint_filepath = os.path.join(BASE_PATH, 'mobilenetv2.hdf5')

  model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(
      filepath=model_checkpoint_filepath,
      save_weights_only=True,
      monitor='val_loss',
      mode='auto',
      save_best_only=True,
      on_epoch_end=gc.collect())

  early_stopping = tensorflow.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0,
    patience=patience,
    verbose=0,
    mode="auto",
    baseline=None,
    restore_best_weights=True,
 )
  # tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
  model.compile(
      optimizer=tensorflow.optimizers.SGD(learning_rate=0.001), # learning_rate=0.001
      loss="categorical_crossentropy",  # sparse_categorical_crossentropy, binary_crossentropy
      metrics=[keras.metrics.CategoricalAccuracy(name="accuracy")],
  )
  model.summary()
  return model, model_checkpoint_callback, model_checkpoint_filepath, early_stopping

def return_model(name, patience=30, classes=2):
  if name == 'alex_net':
    #alex_net.summary()
    model, checkpoint_callback, checkpoint_filepath, early_stopping = create_alex_net(patience=patience, classes=classes)
    return model, checkpoint_callback, checkpoint_filepath, early_stopping
  if name == 'vgg16_net':
    #alex_net.summary()
    model, checkpoint_callback, checkpoint_filepath, early_stopping = create_vgg16_net(patience=patience, classes=classes)
    return model, checkpoint_callback, checkpoint_filepath, early_stopping
  if name == 'leaf_net':
    # karpas_net3.summary()
    return create_leaf_net(patience=patience, classes=classes)
  if name == "inception":
    model, checkpoint_callback, checkpoint_filepath, early_stopping = create_inception_net(patience=patience, classes=classes)
    return model, checkpoint_callback, checkpoint_filepath, early_stopping
  if name == "resnet50":
    model, checkpoint_callback, checkpoint_filepath, early_stopping = create_reset50(patience=patience, classes=classes)
    return model, checkpoint_callback, checkpoint_filepath, early_stopping
  if name == "mobilenetv2":
    model, checkpoint_callback, checkpoint_filepath, early_stopping = create_mobilenetv2(patience=patience, classes=classes)
    return model, checkpoint_callback, checkpoint_filepath, early_stopping
